#!/usr/bin/env python3
"""
ìš©ì ‘ì„  ë°ì´í„° CSV ì¶”ì¶œ ë„êµ¬
ì„¤ì • íŒŒì¼ ê¸°ë°˜ìœ¼ë¡œ í•„ìš”í•œ í•„ë“œë§Œ ì„ íƒí•˜ì—¬ CSV ìƒì„±
"""

from pathlib import Path
import json
import re
import csv
from datetime import datetime
from typing import List, Dict, Any, Optional
import argparse

from parse_field_definitions import ALL_FIELDS, PRESETS, get_field_groups


class DataExtractor:
    """ë¡œê·¸ ë°ì´í„° ì¶”ì¶œê¸°"""

    def __init__(self, config_file: Path):
        with open(config_file, 'r', encoding='utf-8') as f:
            self.config = json.load(f)

        # ì„ íƒëœ í•„ë“œ ê²°ì •
        if self.config.get('preset'):
            preset = self.config['preset']
            if preset in PRESETS:
                self.selected_fields = PRESETS[preset]
            else:
                print(f"âš ï¸  ì•Œ ìˆ˜ ì—†ëŠ” í”„ë¦¬ì…‹: {preset}, custom_fields ì‚¬ìš©")
                self.selected_fields = self.config.get('custom_fields', [])
        else:
            self.selected_fields = self.config.get('custom_fields', [])

        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir = Path(self.config['output_settings']['output_dir'])
        output_dir.mkdir(exist_ok=True)
        self.output_dir = output_dir

    def extract_log_data(self, log_file: Path) -> tuple:
        """ë¡œê·¸ íŒŒì¼ì—ì„œ ë°ì´í„° ì¶”ì¶œ"""
        with open(log_file, 'r', encoding='utf-8') as f:
            content = f.read()

        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        metadata = self._extract_metadata(log_file)

        # ìš©ì ‘ì„  ë°ì´í„° ì¶”ì¶œ
        pattern = r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}),\d+ INFO.*?\[GantryRobotControllerServiceImpl\.RequestWeldingOperationStart\(\)\] ENTERED\. (.+?)(?=\n\d{4}-\d{2}-\d{2}|\Z)'
        matches = re.finditer(pattern, content, re.DOTALL)

        raw_data = []
        for match in matches:
            try:
                timestamp = match.group(1)
                json_str = match.group(2).strip()
                data = json.loads(json_str)

                # íŠ¹ìˆ˜ í•„ë“œ ì¶”ê°€
                data['_timestamp'] = timestamp
                data['_girder'] = metadata['girder']

                raw_data.append(data)
            except (json.JSONDecodeError, KeyError):
                continue

        return raw_data, metadata

    def _extract_metadata(self, log_file: Path) -> Dict:
        """ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        metadata = {
            'log_file': log_file.name,
            'girder': 'ì•Œìˆ˜ì—†ìŒ',
            'date': 'ì•Œìˆ˜ì—†ìŒ'
        }

        # ê²½ë¡œì—ì„œ ê±°ë” ì •ë³´ ì¶”ì¶œ
        parts = log_file.parts
        for part in parts:
            if 'Bë¼ì¸' in part or 'ê±°ë”' in part:
                metadata['girder'] = part
                break

        # ë‚ ì§œ ì¶”ì¶œ
        date_match = re.search(r'(\d{8})', log_file.name)
        if date_match:
            date_str = date_match.group(1)
            try:
                date_obj = datetime.strptime(date_str, '%Y%m%d')
                metadata['date'] = date_obj.strftime('%Y%m%d')
            except:
                metadata['date'] = date_str

        return metadata

    def get_field_value(self, data: Dict, field_def) -> Any:
        """í•„ë“œ ì •ì˜ì— ë”°ë¼ ê°’ ì¶”ì¶œ"""
        path = field_def.path
        robot_index = field_def.robot_index

        # íŠ¹ìˆ˜ í•„ë“œ ì²˜ë¦¬
        if path.startswith('_'):
            return data.get(path)

        # ì¼ë°˜ í•„ë“œ ì²˜ë¦¬
        parts = path.split('.')
        current = data

        for i, part in enumerate(parts):
            if part == 'weldJobs':
                # weldJobs ë°°ì—´ ì²˜ë¦¬
                if 'weldJobs' not in current or robot_index is None:
                    return field_def.default
                if robot_index >= len(current['weldJobs']):
                    return field_def.default
                current = current['weldJobs'][robot_index]
            else:
                if not isinstance(current, dict) or part not in current:
                    return field_def.default
                current = current[part]

        # í¬ë§·íŒ… ì ìš©
        if field_def.formatter and current is not None:
            try:
                return field_def.formatter(current)
            except:
                return current

        return current

    def convert_to_rows(self, raw_data: List[Dict]) -> tuple:
        """ì›ì‹œ ë°ì´í„°ë¥¼ CSV í–‰ìœ¼ë¡œ ë³€í™˜ (ì¤‘ë³µ ì œê±° í¬í•¨)"""
        rows = []
        seen_request_ids = set()
        duplicate_count = 0

        for data in raw_data:
            row = {}
            for field_name in self.selected_fields:
                if field_name not in ALL_FIELDS:
                    print(f"âš ï¸  ì•Œ ìˆ˜ ì—†ëŠ” í•„ë“œ: {field_name}")
                    row[field_name] = None
                    continue

                field_def = ALL_FIELDS[field_name]
                value = self.get_field_value(data, field_def)
                row[field_name] = value

            # RequestID ì¤‘ë³µ ì²´í¬
            if 'requestId' in row and row['requestId'] is not None:
                request_id = row['requestId']
                if request_id in seen_request_ids:
                    duplicate_count += 1
                    continue  # ì¤‘ë³µì´ë©´ ê±´ë„ˆë›°ê¸°
                seen_request_ids.add(request_id)

            rows.append(row)

        return rows, duplicate_count

    def export_to_csv(self, rows: List[Dict], output_file: Path, duplicate_count: int = 0):
        """CSV íŒŒì¼ë¡œ ì €ìž¥"""
        if not rows:
            print(f"  âš ï¸  ë°ì´í„° ì—†ìŒ, íŒŒì¼ ìƒì„± ê±´ë„ˆëœ€: {output_file}")
            return

        settings = self.config['output_settings']

        with open(output_file, 'w', encoding=settings['encoding'], newline='') as f:
            writer = csv.DictWriter(
                f,
                fieldnames=self.selected_fields,
                delimiter=settings['delimiter']
            )

            if settings['include_header']:
                writer.writeheader()

            writer.writerows(rows)

        msg = f"  âœ… {output_file.name} ({len(rows)}í–‰, {len(self.selected_fields)}ì»¬ëŸ¼)"
        if duplicate_count > 0:
            msg += f" [ì¤‘ë³µ ì œê±°: {duplicate_count}í–‰]"
        print(msg)

    def process_log_file(self, log_file: Path):
        """ë‹¨ì¼ ë¡œê·¸ íŒŒì¼ ì²˜ë¦¬"""
        print(f"\nðŸ“‚ ì²˜ë¦¬ ì¤‘: {log_file.relative_to(Path.cwd())}")

        # ë°ì´í„° ì¶”ì¶œ
        raw_data, metadata = self.extract_log_data(log_file)

        if not raw_data:
            print("  âš ï¸  ë°ì´í„° ì—†ìŒ")
            return None

        # CSV í–‰ìœ¼ë¡œ ë³€í™˜ (ì¤‘ë³µ ì œê±°)
        rows, duplicate_count = self.convert_to_rows(raw_data)

        # íŒŒì¼ëª… ìƒì„±
        girder_name = metadata['girder'].replace('Bë¼ì¸', '').replace('ë²ˆê±°ë”', 'ê±°ë”')
        output_file = self.output_dir / f"{girder_name}_{metadata['date']}.csv"

        # CSV ì €ìž¥
        self.export_to_csv(rows, output_file, duplicate_count)

        return {
            'metadata': metadata,
            'rows': rows,
            'duplicate_count': duplicate_count,
            'output_file': output_file
        }

    def process_all_girders(self, base_dir: Path):
        """ì „ì²´ ê±°ë” ì¼ê´„ ì²˜ë¦¬"""
        log_files = self.find_all_log_files(base_dir)

        if not log_files:
            print("âŒ ë¡œê·¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        print(f"\nâœ… ì´ {len(log_files)}ê°œ ë¡œê·¸ íŒŒì¼ ë°œê²¬")

        all_results = []
        all_rows = []
        total_duplicates = 0

        for log_file in log_files:
            result = self.process_log_file(log_file)
            if result:
                all_results.append(result)
                all_rows.extend(result['rows'])
                total_duplicates += result.get('duplicate_count', 0)

        # ì „ì²´ í†µí•© íŒŒì¼ ìƒì„± (ì˜µì…˜)
        if self.config['processing_options'].get('merge_all_girders', False):
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            merged_file = self.output_dir / f"ì „ì²´í†µí•©_{timestamp}.csv"
            # í†µí•© íŒŒì¼ë„ ì¤‘ë³µ ì œê±° í•„ìš”
            merged_rows, merged_dups = self._remove_duplicates_from_rows(all_rows)
            self.export_to_csv(merged_rows, merged_file, merged_dups)
            print(f"\nâœ… í†µí•© íŒŒì¼ ìƒì„±: {merged_file.name}")

        print(f"\n{'='*80}")
        print(f"ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"{'='*80}")
        print(f"ì²˜ë¦¬ëœ íŒŒì¼: {len(all_results)}ê°œ")
        print(f"ì´ ë°ì´í„° í–‰: {len(all_rows):,}ê°œ")
        if total_duplicates > 0:
            print(f"ì¤‘ë³µ ì œê±°: {total_duplicates:,}í–‰")
        print(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {self.output_dir}")

    def _remove_duplicates_from_rows(self, rows: List[Dict]) -> tuple:
        """ì´ë¯¸ ìƒì„±ëœ rowsì—ì„œ ì¤‘ë³µ ì œê±°"""
        unique_rows = []
        seen_ids = set()
        dup_count = 0

        for row in rows:
            if 'requestId' in row and row['requestId'] is not None:
                rid = row['requestId']
                if rid in seen_ids:
                    dup_count += 1
                    continue
                seen_ids.add(rid)
            unique_rows.append(row)

        return unique_rows, dup_count

    def find_all_log_files(self, base_dir: Path) -> List[Path]:
        """ëª¨ë“  rcs_*.log íŒŒì¼ ì°¾ê¸°"""
        log_files = []

        for girder_dir in base_dir.glob('Bë¼ì¸*ê±°ë”'):
            for log_file in girder_dir.rglob('rcs_*.log'):
                if not log_file.name.startswith('rcs_error'):
                    log_files.append(log_file)

        return sorted(log_files)


def show_available_fields():
    """ì‚¬ìš© ê°€ëŠ¥í•œ í•„ë“œ ëª©ë¡ í‘œì‹œ"""
    print("\n" + "="*80)
    print("ì‚¬ìš© ê°€ëŠ¥í•œ í•„ë“œ ëª©ë¡")
    print("="*80)

    groups = get_field_groups()
    for group_name, fields in groups.items():
        print(f"\nðŸ“Œ {group_name}")
        for field_name in fields:
            field_def = ALL_FIELDS[field_name]
            print(f"  - {field_name:25s} : {field_def.description}")

    print("\n" + "="*80)
    print("í”„ë¦¬ì…‹")
    print("="*80)
    for preset_name, fields in PRESETS.items():
        print(f"\nðŸ“¦ {preset_name} ({len(fields)}ê°œ í•„ë“œ)")
        print(f"  {', '.join(fields[:10])}", end='')
        if len(fields) > 10:
            print(f" ... ì™¸ {len(fields)-10}ê°œ")
        else:
            print()


def create_config_template(output_file: Path):
    """ì„¤ì • íŒŒì¼ í…œí”Œë¦¿ ìƒì„±"""
    template = {
        "_comment": "ìš©ì ‘ì„  ë°ì´í„° ì¶”ì¶œ ì„¤ì • íŒŒì¼",
        "_preset_info": "í”„ë¦¬ì…‹: full, basic, coordinates, image_format",
        "preset": "image_format",
        "custom_fields": PRESETS['image_format'],
        "output_settings": {
            "format": "csv",
            "encoding": "utf-8-sig",
            "delimiter": ",",
            "include_header": True,
            "output_dir": "extracted_data"
        },
        "processing_options": {
            "merge_all_girders": False,
            "separate_by_date": True,
            "skip_empty_files": True
        }
    }

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(template, f, indent=2, ensure_ascii=False)

    print(f"âœ… ì„¤ì • íŒŒì¼ í…œí”Œë¦¿ ìƒì„±: {output_file}")


def main():
    parser = argparse.ArgumentParser(
        description='ìš©ì ‘ì„  ë¡œê·¸ ë°ì´í„°ë¥¼ CSVë¡œ ì¶”ì¶œ'
    )
    parser.add_argument(
        '--config',
        type=Path,
        default=Path('parse_config.json'),
        help='ì„¤ì • íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸: parse_config.json)'
    )
    parser.add_argument(
        '--create-config',
        action='store_true',
        help='ì„¤ì • íŒŒì¼ í…œí”Œë¦¿ ìƒì„±'
    )
    parser.add_argument(
        '--list-fields',
        action='store_true',
        help='ì‚¬ìš© ê°€ëŠ¥í•œ í•„ë“œ ëª©ë¡ í‘œì‹œ'
    )
    parser.add_argument(
        '--preset',
        choices=list(PRESETS.keys()),
        help='í”„ë¦¬ì…‹ ì„ íƒ (ì„¤ì • íŒŒì¼ ë¬´ì‹œ)'
    )

    args = parser.parse_args()

    # í•„ë“œ ëª©ë¡ í‘œì‹œ
    if args.list_fields:
        show_available_fields()
        return

    # ì„¤ì • íŒŒì¼ í…œí”Œë¦¿ ìƒì„±
    if args.create_config:
        create_config_template(args.config)
        return

    # ì„¤ì • íŒŒì¼ í™•ì¸
    if not args.config.exists():
        print(f"âŒ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.config}")
        print(f"   í…œí”Œë¦¿ ìƒì„±: python {Path(__file__).name} --create-config")
        return

    # í”„ë¦¬ì…‹ ì˜¤ë²„ë¼ì´ë“œ
    if args.preset:
        with open(args.config, 'r', encoding='utf-8') as f:
            config = json.load(f)
        config['preset'] = args.preset
        with open(args.config, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        print(f"âœ… í”„ë¦¬ì…‹ ë³€ê²½: {args.preset}")

    # ë°ì´í„° ì¶”ì¶œ ì‹¤í–‰
    print("="*80)
    print("ìš©ì ‘ì„  ë°ì´í„° CSV ì¶”ì¶œ")
    print("="*80)
    print(f"ì„¤ì • íŒŒì¼: {args.config}")

    extractor = DataExtractor(args.config)

    print(f"ì„ íƒëœ í•„ë“œ: {len(extractor.selected_fields)}ê°œ")
    print(f"  {', '.join(extractor.selected_fields[:10])}", end='')
    if len(extractor.selected_fields) > 10:
        print(f" ... ì™¸ {len(extractor.selected_fields)-10}ê°œ")
    else:
        print()

    base_dir = Path(__file__).parent
    extractor.process_all_girders(base_dir)


if __name__ == '__main__':
    main()

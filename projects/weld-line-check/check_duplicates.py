#!/usr/bin/env python3
"""
RequestID ì¤‘ë³µ ê²€ì‚¬ ë„êµ¬
"""

import csv
from pathlib import Path
from collections import Counter, defaultdict


def analyze_csv_duplicates(csv_file: Path):
    """CSV íŒŒì¼ì˜ RequestID ì¤‘ë³µ ë¶„ì„"""
    if not csv_file.exists():
        return None

    request_ids = []

    with open(csv_file, 'r', encoding='utf-8-sig') as f:
        reader = csv.DictReader(f)
        for row in reader:
            if 'requestId' in row and row['requestId']:
                request_ids.append(int(row['requestId']))

    total_count = len(request_ids)
    unique_count = len(set(request_ids))
    duplicate_count = total_count - unique_count

    # ì¤‘ë³µ ID ì°¾ê¸°
    id_counts = Counter(request_ids)
    duplicates = {rid: count for rid, count in id_counts.items() if count > 1}

    return {
        'file': csv_file.name,
        'total': total_count,
        'unique': unique_count,
        'duplicate_rows': duplicate_count,
        'duplicate_ids': duplicates
    }


def main():
    base_dir = Path(__file__).parent / 'extracted_data'

    if not base_dir.exists():
        print(f"âŒ {base_dir} í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    print("=" * 80)
    print("RequestID ì¤‘ë³µ ê²€ì‚¬")
    print("=" * 80)
    print()

    csv_files = sorted(base_dir.glob('*.csv'))

    if not csv_files:
        print("âŒ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    all_results = []
    total_rows = 0
    total_unique = 0
    total_duplicates = 0
    all_request_ids = []

    # íŒŒì¼ë³„ ë¶„ì„
    for csv_file in csv_files:
        result = analyze_csv_duplicates(csv_file)
        if result:
            all_results.append(result)
            total_rows += result['total']
            total_unique += result['unique']
            total_duplicates += result['duplicate_rows']

            # ì „ì²´ RequestID ìˆ˜ì§‘
            with open(csv_file, 'r', encoding='utf-8-sig') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    if 'requestId' in row and row['requestId']:
                        all_request_ids.append(int(row['requestId']))

    # íŒŒì¼ë³„ ê²°ê³¼ ì¶œë ¥
    print("ğŸ“‚ íŒŒì¼ë³„ ì¤‘ë³µ í˜„í™©:")
    print()

    has_duplicates = False
    for result in all_results:
        file_name = result['file']
        total = result['total']
        unique = result['unique']
        dup_rows = result['duplicate_rows']
        dup_ids = result['duplicate_ids']

        if dup_rows > 0:
            has_duplicates = True
            print(f"âš ï¸  {file_name}")
            print(f"    ì „ì²´: {total:,}í–‰, ê³ ìœ : {unique:,}ê°œ, ì¤‘ë³µ: {dup_rows:,}í–‰")
            print(f"    ì¤‘ë³µ ID ëª©ë¡:")
            for rid, count in sorted(dup_ids.items(), key=lambda x: x[1], reverse=True)[:5]:
                print(f"      - RequestID {rid}: {count}íšŒ ì¶œí˜„")
            if len(dup_ids) > 5:
                print(f"      ... ì™¸ {len(dup_ids) - 5}ê°œ")
            print()
        else:
            print(f"âœ… {file_name}: {total:,}í–‰ (ì¤‘ë³µ ì—†ìŒ)")

    print()
    print("=" * 80)
    print("ğŸ“Š ì „ì²´ í†µê³„:")
    print("=" * 80)
    print(f"ì´ íŒŒì¼: {len(all_results)}ê°œ")
    print(f"ì´ ë°ì´í„°: {total_rows:,}í–‰")
    print(f"ê³ ìœ  RequestID: {total_unique:,}ê°œ")
    print(f"ì¤‘ë³µ ë°ì´í„°: {total_duplicates:,}í–‰ ({total_duplicates/total_rows*100:.1f}%)")
    print()

    # ì „ì²´ ë°ì´í„°ì—ì„œ íŒŒì¼ ê°„ ì¤‘ë³µ í™•ì¸
    all_id_counts = Counter(all_request_ids)
    cross_file_duplicates = {rid: count for rid, count in all_id_counts.items() if count > 1}

    print("ğŸ” íŒŒì¼ ê°„ ì¤‘ë³µ (ê°™ì€ RequestIDê°€ ì—¬ëŸ¬ íŒŒì¼ì— ì¡´ì¬):")
    print()

    if cross_file_duplicates:
        # RequestIDë³„ë¡œ ì–´ëŠ íŒŒì¼ì— ìˆëŠ”ì§€ í™•ì¸
        id_to_files = defaultdict(list)

        for csv_file in csv_files:
            with open(csv_file, 'r', encoding='utf-8-sig') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    if 'requestId' in row and row['requestId']:
                        rid = int(row['requestId'])
                        if rid in cross_file_duplicates:
                            id_to_files[rid].append(csv_file.name)

        # ìƒìœ„ 5ê°œë§Œ ì¶œë ¥
        for rid, count in sorted(cross_file_duplicates.items(), key=lambda x: x[1], reverse=True)[:5]:
            files = list(set(id_to_files[rid]))
            print(f"  RequestID {rid}: ì´ {count}íšŒ")
            print(f"    íŒŒì¼: {', '.join(files[:3])}", end='')
            if len(files) > 3:
                print(f" ì™¸ {len(files)-3}ê°œ")
            else:
                print()

        if len(cross_file_duplicates) > 5:
            print(f"  ... ì™¸ {len(cross_file_duplicates) - 5}ê°œ ì¤‘ë³µ ID")
    else:
        print("  âœ… íŒŒì¼ ê°„ ì¤‘ë³µ ì—†ìŒ")

    print()
    print("=" * 80)
    print("ğŸ’¡ ê¶Œì¥ì‚¬í•­:")
    print("=" * 80)

    if has_duplicates or cross_file_duplicates:
        print("âš ï¸  RequestID ì¤‘ë³µì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print()
        print("í•´ê²° ë°©ì•ˆ:")
        print("  1. ì¤‘ë³µ ì œê±°: ê°™ì€ RequestIDëŠ” ì²« ë²ˆì§¸ë§Œ ìœ ì§€")
        print("  2. ì¤‘ë³µ í‘œì‹œ: CSVì— 'is_duplicate' ì»¬ëŸ¼ ì¶”ê°€")
        print("  3. í†µê³„ ë³´ì •: ê²€ì¦ ì‹œ unique RequestIDë§Œ ì¹´ìš´íŠ¸")
        print()
        print("íŒŒì‹± ìŠ¤í¬ë¦½íŠ¸ ìˆ˜ì • í•„ìš”:")
        print("  - parse_to_csv.py: ì¤‘ë³µ ê°ì§€ ë° ì²˜ë¦¬")
        print("  - analyze_logs_excel.py: í†µê³„ ê³„ì‚° ì‹œ unique ì¹´ìš´íŠ¸")
    else:
        print("âœ… ì¤‘ë³µì´ ì—†ìŠµë‹ˆë‹¤. ì¶”ê°€ ì¡°ì¹˜ ë¶ˆí•„ìš”.")

    print("=" * 80)


if __name__ == '__main__':
    main()
